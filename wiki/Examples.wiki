#summary Describes example MRToolkit jobs.

= Examples =

The {{{examples}}} directory contains some samples jobs.  These were designed to illustrate various constructs and techniques.

== ip.rb ==
This is the simplest example, containing only a job class.  It uses the pre-written !CopyMap, which simply copies its input to the output with no processing.  The pre-written reducer !UniqueCountReduce counts the number of each unique instance of the first field, and outputs the field and its count.  

Hadoop, and MRToolkit, leaves the reducer output in HDFS, in as many files as there were reduce jobs.  Because it is not particularly useful in this form, a subsequent processing step ordinarily copies all the reduce output files to the local file system, then concatenates and sorts them.  These steps are done in the included Rakefile tasks.

In this case we want to sort on the second field (the count) so we see the IP addresses that generated the most hits.

== section.rb ==
This job extracts the section name (the first directory segment of the requested path) and counts the occurrences of each one.

The mapper process step extracts the section name and adds a count (1).  The pre-written reducer !UniqueSumReduce sums these up.  This example could have omitted the count and used !UniqueCountReduce, but was done this way to introduce the sum reducer.

== ip-size.rb ==
This job adds up the size of all the results returned to each distinct IP address.

It illustrates how to define input and output fields.  Its process step picks up selected input fields and places them in the output.  This is pretty typical: the mapper is often used to select and rearrange the input fields.  Remember that the output of the mapper is shuffled, so that all records with the same first field are fed to the (same) reducer together.

Here !UniqueSumReduce actually has something to add up (rather than just counting as in the last example).  

== hour.rb ==
This job summaries the traffic by hour.  It counts the requests that occur in each hour, across all the days.  

This illustrates an atypical mapper.  A more straightforward implementation would have extracted and output just the hour.  Then !UniqueCountReduce could have been used to count up each of the numbers (0-23).

In the implementation given, however, the mapper creates an array of 24 counters, and it's process function increments one of these counters for each input record.  The function returns nil, so no output is produced yet.  At the end, these arrays are written out.  

So each mapper writes only 24 lines of output.  The reducer needs only sum these up to get the final result, which it again uses !UniqueSumReduce to do.  

The functions process_begin and process_end are used to (1) initialize the counter array, and (2) write the mapper output.  You should only define these functions if you need them.

== top-files.rb ==
This job finds the top 10 most frequently requested files (using the full request path).  

The map stage by this time should be familiar: it simply picks out the path and adds a count of 1.

The reducer, !MaxUniqueSumReduce, is a version of !UniqueSumReduce which only retains a given number of values (given by a parameter, in this case 10).  By retaining only the largest counts, the output is substantially reduced.  

Because Hadoop may use many reducers, and each operates independently, each will output a different 10 highest file-count pairs.  These are not guaranteed to be the highest overall, since each reducer sees different pairs.  But the output _is_ guaranteed to have to 10 highest counts overall.

== import.rb ==
This is not a Hadoop job, but an import program for Apache log files.
It takes a raw file on standard input and produces a clean up and delimited one on standard output.  The shell program {{{import-logs}}} creates a directory in HDFS, feeds each of the raw log files into import.rb, and stores the result in HDFS.

The import program simply uses a regular expression to parse the log file.